# Abstract

FlexGen，这是一个用于在有限 GPU 内存下运行 LLM 的高吞吐量生成引擎。FlexGen 可以根据不同的硬件资源约束灵活配置，通过聚合 GPU、CPU 和磁盘的内存和计算能力来实现高效运行。

==通过求解线性规划问题，FlexGen 搜索存储和访问张量的高效模式，并进一步将权重和注意力缓存压缩到 4 位，几乎不损失精度。==

“offloading”:一种**计算和内存卸载技术**，即将部分计算任务或数据从一个设备（如 GPU）转移到其他设备（如 CPU 或磁盘）以优化资源利用和性能。

HELM benchmark（Holistic Evaluation of Language Models）：一个专门用于评估 **大语言模型（LLMs）全面能力** 的测试基准。相关论文是 *Liang et al., 2022*。**HELM 的目标是全面而系统地评估语言模型的真实表现**，从多个维度上衡量

# 1、Introduction

**Latency：单个请求响应的时间**

**Throughput：单位时间内处理的任务数量(单位requests/sec, tokens/sec)**

**Offloading**：一种计算和内存卸载技术

粗粒度量化：对整个权重矩阵进行量化处理

细粒度量化：将权重矩阵分成多个小组，每个小组分别进行量化，这样可以减少精度的损失

运行大模型需要极高的计算和内存需求。例如，GPT-175B 需要 325GB 的 GPU 内存才能加载其模型权重。要将这个模型部署到 GPU 上，至少需要五块 A100（80GB）GPU，并且需要复杂的并行策略（Pope et al., 2022; Aminabadi et al., 2022）

本文研究**throughpu toriented generative inference**（**吞吐量导向的生成推理**）

延迟敏感性低：**精度优先，响应可等**。例如：LLMs中的基准测试（Liang et al., 2022）、信息提取（Narayan et al., 2018）、数据整理（Narayan et al., 2022）和表单处理（Chen et al., 2021）

以往研究的三个方向：

* （1）通过模型压缩减少总内存占用（Dettmers et al., 2022; Yao et al., 2022; Frantar et al., 2022; Xiao et al., 2022）；
* （2）通过去中心化进行协作推理，以分摊推理成本（Borzunov et al., 2022）；以及
* （3）通过将数据卸载到 CPU 和磁盘来利用内存（Aminabadi et al., 2022; HuggingFace, 2022）

第三类中现有的基于卸载的系统在单个 GPU 上无法实现可接受的吞吐量，因为它们的 I/O 调度和张量放置效率低下。

本文的重点：为单个普通 GPU 上的高吞吐量生成推理设计高效的卸载策略。

在吞吐量导向的场景中，我们可以通过使用大批次大小来牺牲延迟，并在不同的内存层次结构之间分摊昂贵的 I/O 操作，使其覆盖大量的输入批次。

牺牲Latency的前提下，在有限的GPU memory下实现高thoughput的挑战：

* 设计**高效的卸载策略**。在生成式推理过程中，存在三种类型的张量：**权重、激活值和键值（KV）缓存**。该策略应指定要卸载哪些张量，将它们卸载到三级内存层次结构中的哪个位置，以及在推理过程中何时进行卸载。计算的batch by batch, token by token, layer by layer结构形成了一个复杂的依赖图，其中存在多种执行计算的方式。这些选择共同构成了一个复杂的设计空间。
* 开发**有效的压缩策略**。以往的研究已经在压缩 LLM 的权重和激活值方面取得了有希望的结果。然而，当将压缩与卸载结合用于高吞吐量推理时，权重和 KV 缓存的 I/O 成本和内存减少变得更加重要，这促使我们探索替代的压缩方案。

**Contribution1**：考虑计算调度、张量放置和计算委托(computation delegation)，正式定义了可能的卸载策略的搜索空间。我们证明我们的搜索空间捕获了一个 I/O 复杂度在 最优的计算顺序的2 倍以内。然后，我们==开发了一种基于线性规划的搜索算法==，以优化搜索空间内的吞吐量。**该算法可以针对各种硬件规格进行配置，并且可以轻松扩展以包含延迟和吞吐量约束，从而帮助平滑地导航权衡空间。**与现有策略相比，我们的解决方案统一了权重、激活值和 KV 缓存的放置，使得可以显著提高最大批量大小上限，这是实现高吞吐量的关键。

**Contribution2**：展示了可以将像 OPT-175B 这样的 LLM 的权重和 KV 缓存压缩到 4 位，而无需重新训练或校准，并且几乎不损失精度。这是通过细粒度的分组量化（Shen et al., 2020）实现的，这对于减少卸载过程中的 I/O 成本和内存使用非常合适。

**Contribution3**：与 DeepSpeed Zero-Inference（Aminabadi et al., 2022）和 Hugging Face Accelerate（HuggingFace, 2022）这两个基于卸载的最先进的推理系统相比，**FlexGen 通常允许使用大得多的批量大小**。因此，FlexGen 可以实现更高的吞吐量。

# 2、Relate Work

现有的LLM推理系统大多专注于使用高端加速器的低延迟场景，限制了它们在易于获取的硬件上用于吞吐量导向推理的部署。

为了在普通硬件上启用 LLM 推理，卸载是一种关键技术。现有的推理系统通常继承了训练系统的卸载技术（Rajbhandari et al., 2021; Ren et al., 2021; Li et al., 2022; Huang et al., 2020; Wang et al., 2018），但忽略了生成式推理的特殊计算属性。

**以算法为导向的工作relax certain aspects of computation**(放宽某些具体方面的计算要求):

* 在精度方面，使用FP32浮点数改为FP16，INT8混合精度计算
* 注意力计算方面，全量注意力（O($n^2$)）改为稀疏注意力、局部注意力
* KV Cache更新方面，每一步更新完整缓存改为滑动窗口或者只更新一部分
* Transformer层计算方面，对所有Transformer层的全计算改为Layer Skipping，Early Exit
* 序列处理方面，长序列直接处理改为分段处理，分块计算

**将权重压缩到三位**：原来的权重可能使用常规的16位（FP16）浮点数，占用了很多的内存和计算资源，3位bit位可以表示8种不同的数值，我们可以使用类似K-means算法学习8个预设值，对某个权重进行压缩的操作就是把它近似为预设值其中的一个，同时也可以选择压缩到4位或者其他位。

KV-Cache：一种缓存机制，用于存储 Transformer 模型中每一层的“Key”和“Value”向量，以避免重复计算，从而**加快推理速度，特别是在生成长文本时**。

# 3、backgound：LLM Inference

隐藏维度h1表示token被embedding之后的维度，h2表示在前馈网络（FFN）中隐藏层的维度

详见transformer的笔记

# 4、offloading strategy

## 4.1、问题公式化

**Prompt（提示）**：指的是你输入给大语言模型的一段**文本**，用于引导模型产生你想要的输出。prompt会被tokenizer转换成一串token

在图2中，一个正方形表示一个GPU批次对于一层的计算。具有相同颜色的的正方形共享相同的权重。定义一个有效路径为一个遍历（计算）所有正方形的路径，同时满足一下约束条件：

* 只有当同一行左侧的所有正方形都被计算过时，一个正方形才能被计算。
* 要在某个设备上计算一个正方形，其所有输入（权重、激活值、缓存）都必须被加载到同一个设备上。
* 在计算一个正方形后，它会产生两个输出：激活值和 KV 缓存。激活值应被存储，直到其右侧的兄弟正方形被计算。KV 缓存应被存储，直到同一行最右侧的正方形被计算。
* 在任何时候，一个设备上存储的张量总大小不能超过其内存容量。

第三点中的activation（激活值）指的是模型在反向传播中需要用到的数据

目标是找到一个有效路径，以最小化总执行时间，这包括在设备间移动张量时的计算成本和I/O成本。

## 4.2、搜索空间

所有现有系统（Aminabadi et al., 2022; HuggingFace, 2022）都按行顺序遍历图，这是完成一个批次生成的最快方式，且 KV 缓存可以在完成一行后立即被释放。然而，由于连续的两个正方形不共享权重，这种调度方式必须反复加载权重，从而产生巨大的 I/O 成本。

为了减少权重的 I/O 成本，我们可以按列顺序遍历图。同一列中的所有正方形共享权重，因此我们可以让权重留在 GPU 上以便重用，而只需加载和卸载激活值和 KV 缓存。然而，我们不能将一列一直计算到底，因为激活值和 KV 缓存仍需被存储。因此，我们必须在它们填满 CPU 和磁盘内存时停止。

在图2中，假设每个LLM都有4层，每层都是一个transformer，经过4层之后产生一个新的token，这个token和原来的所有token组合成新的prompt，重新输入给LLM产生更新的token。在图3中，一个GPU batch包含产生3个token的所有transformer结构。

按列顺序遍历图，以让权重留在 GPU 上以便重用。==然而，我们不能将一列一直计算到底，因为激活值和 KV 缓存仍需被存储。==

**定理 4.1**：之字形块调度的 I/O 复杂度在最优解的 2 倍以内。

**处理LLM时的分块**：将很长的输入序列（如一篇几千词的文章）分成若干小段（chunks）进行处理

**原生Transformer Block**:

* 多头注意力层（Multi-Head Attention）

* Add & Norm

* 前馈神经网络（Feedforward Layer）

* Add & Norm

**GPU batches**（有时也叫 **GPU chunks** 或 **GPU blocks**）是指：将**分块后的数据组织成适合 GPU 并行处理的小批次（batch）**，让多个 token 或多个序列 **同时送入 GPU 中执行计算**，提高吞吐量、节省计算资源。一个GPU batch中包含多个chunks

一般的Attention机不能跨chunk感知token

overlapping optimization algorithm中，GPU 批次大小和块中 GPU 批次数量的乘积称为块大小（或有效批次大小）

张量并行：将权重矩阵分成更小的部分，分别在多个不同的GPU中进行运算。常用在多头注意力和MLP层

流水线并行：将模型分成几个“阶段”（layer级别的切分），每个GPU处理一部分分层，像工厂流水线那样。

对于流水线并行，虽然后面层的计算会用到前面层的输出，但是由于我们同时处理了多条prompt，当GPU0处理某条prompt前面的layer时，GPU1可以处理当前prompt前面的prompt的后面的layer。通过这种方式，GPU不用过多等待需要被处理的数据

# 5、Approximate Methods

## 分组量化

可以将权重和 KV 缓存直接量化为 4 位整数，而无需对 OPT-175B 进行任何重新训练或校准，同时保持相似的精度（第 6.2 节）。与一些相关工作（Yao et al., 2022; Dettmers et al., 2022; Xiao et al., 2022）相比，这些工作主要使用整数矩阵乘法以加速计算，而我们量化的主要目标是压缩和减少 I/O 成本。

给定一个张量，我们选择沿某个维度的 *g* 个连续元素作为一个组。对于每个组，我们计算该组元素的最小值和最大值，并将每个元素 *x* 量化为 *b* 位整数，公式为
$$
x_{\text{quant}} = \text{round} \left( \frac{x - \text{min}}{\text{max} - \text{min}} \times (2^b - 1) \right)
$$
round操作是指将一个浮点数四舍五入到最接近的整数。

有多种方式可以选择沿哪个维度进行分组。我们发现，沿权重的输出通道维度和 KV 缓存的隐藏维度进行分组，可以在实践中保持精度的同时提高运行时效率。一项concurrent work（Dettmers & Zettlemoyer, 2022）也发现，对于 OPT 模型，4 位精度几乎是最优的总模型位数和零样本精度。

## 稀疏注意力

可以通过仅加载 OPT-175B 的前 10% 注意力值缓存来利用自注意力的稀疏性，同时保持模型质量。在计算注意力矩阵后，对于每个查询，我们计算其来自 K 缓存的 Top-K 个标记的索引。然后我们简单地丢弃其他标记，并根据索引仅加载 V 缓存的一个子集。

# 6、Evaluation

## Workload

在prompt（模型的输入序列）分别为512和1024的情况下进行实验，每次产生32个token

对于所有系统，我们在吞吐量基准测试中使用虚拟模型权重，在精度评估中使用真实权重。

## baseline

使用 DeepSpeed ZeRO-Inference（Aminabadi et al., 2022）和 Hugging Face Accelerate（HuggingFace, 2022）作为基线。它们是仅有的在 GPU 内存不足时支持卸载的 LLM 推理系统。

**ZeRO（Zero Redundancy Optimizer）数据并行**：将模型的状态分布在多个 GPU 上，而不是每个 GPU 都保存一份完整副本，从而节省内存、提升训练规模。

**Accelerate** 支持卸载一部分权重。它不支持跨不同机器的分布式 GPU。

去中心化协作推理是另一种降低 LLM 推理资源需求的选项。因此，我们还将 Petals（Borzunov et al., 2022; Ryabinin et al., 2023）作为额外的基线。

policy search：

* 计算调度：块调度，之字形块调度
* 张量放置：权重放置，激活值放置，KV Cache放置
* 计算委托：CPU计算





















